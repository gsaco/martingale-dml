% ============================================================================
% Martingale Double Machine Learning for Adaptive Experiments
% Q1 Journal Submission (Econometrica / Review of Economic Studies style)
% ============================================================================
\documentclass[12pt]{article}

% ---------- Core Packages ----------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\singlespacing

% ---------- Math Packages ----------
\usepackage{amsmath,amssymb,amsthm,mathtools,bm}
\usepackage{bbm}

\numberwithin{equation}{section}

% ---------- Tables / Figures ----------
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
% \usepackage{multirow}
% \usepackage{longtable}
% \usepackage{float}

% ---------- Lists ----------
\usepackage{enumitem}

% ---------- Citations ----------
\usepackage{natbib}
\setcitestyle{authoryear,round,semicolon}

% ---------- Hyperlinks ----------
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=green!50!black,
    urlcolor=blue!70!black
}

% ---------- Enhanced Theorems ----------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}{Assumption}
\renewcommand{\theassumption}{\Alph{assumption}}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% ---------- Commands ----------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\indep}{\perp\!\!\!\perp}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\norm}[1]{\left\lVert #1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\given}{\,|\,}
\newcommand{\op}{o_{\Prob}(1)}
\newcommand{\Op}{O_{\Prob}(1)}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\pto}{\xrightarrow{\Prob}}

\newcommand{\pit}{\pi_t}
\newcommand{\thetao}{\theta_0}
\newcommand{\thetahat}{\widehat{\theta}}
\newcommand{\Vhat}{\widehat{V}}
\newcommand{\mhat}{\widehat{m}}
\newcommand{\mstar}{m^\star}
\newcommand{\mhatone}{\widehat m_{t-1,1}}
\newcommand{\mhatzero}{\widehat m_{t-1,0}}

\newcommand{\expit}{\operatorname{expit}}
\newcommand{\logit}{\operatorname{logit}}
\newcommand{\clip}{\operatorname{clip}}

% ---------- Header ----------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\footnotesize\textsc{Self-Normalized Martingale DML for Adaptive Experiments}}
\fancyhead[R]{\footnotesize\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\setlength{\headheight}{14pt}

% ============================================================================
\begin{document}

% ============================================================================
% TITLE PAGE
% ============================================================================
\begin{center}
{\LARGE\bfseries Self-Normalized Martingale DML for\\[0.2cm]
Adaptive Experiments with Logged Propensities\par}
\vspace{0.8cm}
{\large Anonymous\par}
\vspace{0.2cm}
{\normalsize MIT Department of Economics\par}
\vspace{0.5cm}
{\normalsize Draft: January 2026\par}
\end{center}

\vspace{0.5cm}

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
\noindent
We study fixed-horizon inference for the average treatment effect (ATE) in
sequential adaptive experiments where treatment probabilities are \emph{logged}
and \emph{predictable} (measurable w.r.t.\ past data and current covariates).
We use the equal-weight AIPW/DML estimator with nuisance predictions constructed
\emph{predictably} (e.g.\ forward cross-fitting or online updating).
Predictability makes the orthogonal score increments an exact martingale
difference sequence, yielding finite-sample (history-conditioned) unbiasedness.

For inference we studentize by the realized quadratic variation, implemented by
the sample variance of the predictable score path, and apply a self-normalized
martingale CLT. As $n_{\mathrm{eff}}\to\infty$, this yields asymptotically valid
Wald confidence intervals under design-stage overlap, predictability, and mild
moment/nondegeneracy conditions, without requiring propensity stabilization
($\pi_t\to\pi$) or nuisance convergence rates for \emph{validity}. Under $L^2$
nuisance consistency we obtain oracle equivalence, and a conditional variance
decomposition implies nuisance error can only inflate uncertainty.

\medskip
\noindent\emph{Scope.} All results are fixed-horizon and assume logged, predictable propensities; Appendix~\ref{app:multiarm} sketches the multi-arm extension.

\medskip
\noindent\textbf{Keywords:} adaptive experiments; AIPW; double machine learning; martingale CLT; self-normalization; sequential randomization.

\smallskip
\noindent\textbf{JEL Codes:} C14, C22, C93.
\end{abstract}

\vspace{0.5cm} % spacer after abstract (ToC removed for page budget)

% ============================================================================
% 1. INTRODUCTION
% ============================================================================
\section{Introduction}\label{sec:intro}

Adaptive experiments choose treatment probabilities using the evolving history,
so the resulting observations are not i.i.d.\ and standard cross-fitted DML
arguments (which rely on approximate independence between nuisance fitting and
score evaluation) do not apply verbatim. We consider fixed-horizon inference for
the ATE when the executed assignment probabilities are logged and predictable.

Our key device is \emph{predictability discipline}: nuisance regressions used to
score unit $t$ are constructed using only past data. This restores an exact
martingale difference structure for the orthogonal AIPW/DML score increments,
which in turn enables inference via self-normalized martingale limit theory.
Studentizing by realized quadratic variation yields valid Wald intervals even
when the adaptive propensity path $\{\pi_t\}$ is nonstationary and does not
stabilize.

\noindent\textbf{Core claim.}
Under sequential randomization with logged propensities that equal the executed
assignment probabilities (Definition~\ref{def:propensity} and Assumption~\ref{ass:unconf}),
design-stage overlap (Assumption~\ref{ass:overlap}), predictable nuisance
construction (Assumption~\ref{ass:predictable_nuis}), and mild moment/nondegeneracy
conditions (Assumptions~\ref{ass:moments} and~\ref{ass:var_growth}), the equal-weight
AIPW/DML score increments form an exact martingale difference array
(Lemma~\ref{lem:mds}). Studentizing by the realized quadratic variation, implemented
by the sample-variance factor $\widehat V$, yields fixed-horizon Wald inference for
the ATE via a self-normalized martingale CLT (Theorem~\ref{thm:studentized}).
Validity does \emph{not} require propensity stabilization ($\pi_t\to\pi$) nor
nuisance convergence rates; nuisance learning affects only precision and oracle
comparisons.

Predictability is not a modeling abstraction: in forward cross-fitting, fitting
on past blocks and freezing the fitted nuisance object within each scored block
provides an auditable sufficient condition (Lemma~\ref{lem:operational_pred}).

\subsection{Contributions}\label{sec:contrib}
We make three contributions. (i) Under sequential randomization and predictable
nuisance construction, the AIPW/DML increment is an exact martingale difference,
giving finite-sample unbiasedness (Lemma~\ref{lem:mds} and
Corollary~\ref{cor:unbiased}). (ii) Studentizing by realized quadratic variation
yields fixed-horizon Wald inference without propensity stabilization and without
nuisance-rate conditions for validity (Theorem~\ref{thm:studentized}). (iii)
Optional precision add-ons include a conditional variance decomposition
(Proposition~\ref{prop:var_decomp}), oracle equivalence under $L^2$ nuisance
consistency (Theorem~\ref{thm:oracle_equiv}), and a conventional CLT under
variance stabilization (Corollary~\ref{cor:nonstudentized}).

\subsection{What This Paper Does and Does Not Claim}\label{sec:scope}

\paragraph{Guarantees.}
Under logged predictable propensities, overlap, and predictable nuisance construction (Assumption~\ref{ass:predictable_nuis}), we prove studentized fixed-horizon asymptotic normality with a feasible self-normalizer (Theorem~\ref{thm:studentized}). As optional add-ons, under variance stabilization we recover a conventional (non-studentized) CLT with deterministic asymptotic variance and consistent variance estimation (Corollary~\ref{cor:nonstudentized}), and under $L^2$ nuisance consistency we obtain oracle equivalence (Theorem~\ref{thm:oracle_equiv}).

\paragraph{Non-guarantees.}
We do \emph{not} provide anytime-valid inference / confidence sequences (see \citealt{cook2024semiparametric}); we require enforced design-stage overlap (post-hoc truncation changes the estimand); we treat $\pi_t$ as known and logged rather than estimated (see \citealt{kato2020adaptive} for settings with unknown propensities); and we do not allow interference or dependence beyond i.i.d.\ unit arrivals. All results are fixed-horizon at a pre-specified terminal time $n$.

\subsection{Related Work}\label{sec:related}
Our note sits between i.i.d.\ DML and adaptive-experiment martingale inference.
DML provides orthogonal-score inference for treatment effects with high-dimensional
covariates under i.i.d.\ sampling \citep{chernozhukov2018double,chernozhukov2022locally}.
Adaptive-experiment analyses with logged propensities use martingale CLTs to
justify fixed-horizon inference for AIPW-type estimators, often under conditions
leading to a deterministic asymptotic variance \citep{kato2020adaptive,cook2024semiparametric}.

Relative to this literature, our novelty is the \emph{self-normalized} route:
predictability of nuisance fits yields exact martingale differences for the
orthogonal score increments, and studentizing by realized quadratic variation
delivers fixed-horizon Wald inference without requiring propensity stabilization
and without requiring nuisance convergence rates for validity. Self-normalized
martingale limit theory is classical (e.g., \citealp{delapena2009self,hall2014martingale});
our contribution is to connect it to predictable AIPW/DML scoring under adaptive
experiments with logged propensities. Formally, our approach avoids imposing
variance stabilization $V_{\mathcal{T}}^2/n_{\mathrm{eff}}\pto V$ (Assumption~\ref{ass:stab});
when this ratio remains random, fixed-variance Wald approximations can fail,
motivating self-normalization (Section~\ref{sec:simulation}). This contrasts with
stabilization-based CLTs in adaptive AIPW analyses \citep{kato2020adaptive,cook2024semiparametric}
and with approaches that modify estimators via adaptive weighting to restore
normality when propensities do not stabilize \citep{hadad2021confidence,zhan2021off}.

\medskip
\noindent\textbf{Additional context.}
For background on response-adaptive randomization and bandit-style trial design see
\citet{rosenberger2015randomization} and \citet{villar2015bandit}; an early econometrics
discussion of adaptive assignment for policy choice is \citet{hahnHiranoKarlan2011}.
See also inference for batched bandits and related sequential-decision asymptotics
\citep{zhang2020inference,hiranoPorter2023asymptotics,liOwen2024batch}.

\paragraph{Organization.} Section~\ref{sec:setup} introduces the model and assumptions.
Section~\ref{sec:score} develops the predictable AIPW/DML estimation framework.
Section~\ref{sec:main_results} presents the main theoretical results with proofs.
Section~\ref{sec:implementation} provides a brief checklist and diagnostics.
Section~\ref{sec:conclusion} concludes.

% ============================================================================
% 2. MODEL AND ASSUMPTIONS
% ============================================================================
\section{Model and Assumptions}\label{sec:setup}

\subsection{Sequential Experimental Design}\label{sec:filtration}

Consider a sequential experiment with fixed horizon $n \in \N$. All randomness is defined on a probability space $(\Omega, \cA, \Prob)$. For each unit $t = 1, \ldots, n$:
\begin{enumerate}[leftmargin=2em, label=\arabic*.]
\item Covariates $X_t \in \cX \subseteq \R^d$ are observed.
\item Treatment $A_t \in \{0,1\}$ is assigned with probability depending on the history.
\item Outcome $Y_t \in \R$ is observed.
\end{enumerate}

Let $Z_t = (X_t, A_t, Y_t, \pi_t)$ denote the observed log record at time $t$,
where $\pi_t$ is the executed assignment probability used to randomize $A_t$.
Define the filtration
\begin{equation}\label{eq:Ft}
\cF_t = \sigma(Z_1, \ldots, Z_t), \qquad \cF_0 = \{\emptyset, \Omega\},
\end{equation}
and the pre-treatment $\sigma$-field
\begin{equation}\label{eq:Gt}
\cG_t = \sigma(\cF_{t-1}, X_t, \pi_t).
\end{equation}

\begin{definition}[Adaptive (logged) propensity]\label{def:propensity}
At time $t$, after observing $(\cF_{t-1},X_t)$, the experimenter chooses an
assignment probability $\pi_t\in(0,1)$ and draws
\[
A_t\mid \cG_t \sim \mathrm{Bernoulli}(\pi_t).
\]
The realized $\pi_t$ is recorded in the log (``known propensities'') and must equal the probability actually passed to the randomization device that generated $A_t$ (see Appendix~\ref{app:repro}). We treat $\pi_t$ as observed and do not estimate propensities in this note. By construction, $\pi_t=\Prob(A_t=1\mid X_t,\cF_{t-1})$ almost surely.
\end{definition}

\begin{remark}[Predictability]\label{rem:predictability}
The propensity $\pit$ is $\cG_t$-measurable: it is determined after observing
current covariates $X_t$ and all prior data $\cF_{t-1}$, but before observing
$(A_t, Y_t)$. This temporal structure is the foundation for our martingale
analysis.
\end{remark}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
Symbol & Meaning \\
\midrule
$n$, $t$ & fixed horizon; unit index $t=1,\dots,n$ \\
$Z_t$ & logged record $(X_t,A_t,Y_t,\pi_t)$ \\
$\cF_t$, $\cG_t$ & post-outcome history; pre-treatment info \eqref{eq:Ft}--\eqref{eq:Gt} \\
$\pi_t$ & logged propensity; $\Prob(A_t=1\mid \cG_t)=\pi_t$ \\
$Y_t(a)$, $Y_t$ & potential outcomes; observed $Y_t=A_tY_t(1)+(1-A_t)Y_t(0)$ \\
$m_a^\star$ & true regression $x\mapsto\E[Y(a)\mid X=x]$ \\
$\widehat m_{t-1,a}$ & predictable nuisance used at time $t$ (Ass.~\ref{ass:predictable_nuis}) \\
$\phi_t(m)$, $\widehat\phi_t$ & pseudo-outcome \eqref{eq:phi_def}; $\widehat\phi_t=\phi_t(\widehat m_{t-1})$ \\
$\theta_0$, $\widehat\theta$ & ATE \eqref{eq:theta0}; estimator \eqref{eq:theta_hat} \\
$\xi_t$, $S_{\mathcal{T}}$ & increment $\xi_t=\widehat\phi_t-\theta_0$; sum $S_{\mathcal{T}}=\sum_{t\in\mathcal{T}}\xi_t$ \\
$\mathcal{T}$, $n_{\mathrm{eff}}$ & scored index set; $n_{\mathrm{eff}}:=|\mathcal{T}|$ \\
$Q_{\mathcal{T}}$, $V_{\mathcal{T}}^2$, $\widehat V$ & realized/predictable QV \eqref{eq:quadratic_variations}; studentizer \eqref{eq:Vhat_def} \\
\bottomrule
\end{tabular}
\caption{Notation (non-exhaustive).}
\label{tab:notation}
\end{table}

\subsection{Potential Outcomes and Target Parameter}\label{sec:estimand}

We adopt the potential outcomes framework. Let $Y_t(a)$ denote the potential outcome under treatment $a \in \{0,1\}$. The observed outcome satisfies $Y_t = A_t Y_t(1) + (1-A_t) Y_t(0)$.

The target parameter is the \emph{average treatment effect} (ATE):
\begin{equation}\label{eq:theta0}
\thetao = \E[Y(1) - Y(0)] = \E[\tau(X)],
\end{equation}
where $\tau(x) = \E[Y(1) - Y(0) \given X = x]$ is the conditional average treatment effect (CATE).

\subsection{Assumptions}\label{sec:assumptions}

We maintain the following assumptions throughout.

\begin{assumption}[Superpopulation i.i.d.\ arrivals]\label{ass:iid}
The sequence $\{(X_t, Y_t(0), Y_t(1))\}_{t=1}^n$ is i.i.d.\ across $t$.
\end{assumption}

\noindent
Let $(X,Y(0),Y(1))$ denote a generic draw from the superpopulation distribution in Assumption~\ref{ass:iid}; then $(X_t,Y_t(0),Y_t(1))\overset{i.i.d.}{\sim}(X,Y(0),Y(1))$ for $t=1,\dots,n$.

\begin{assumption}[Sequential randomization / no anticipation]\label{ass:unconf}
For all $t$,
\[
A_t \indep \{Y_t(0),Y_t(1)\}\mid \cG_t,
\qquad\text{and}\qquad
A_t\mid \cG_t \sim \mathrm{Bernoulli}(\pi_t),
\]
where $\pi_t$ is the logged assignment probability from Definition~\ref{def:propensity}.
\end{assumption}

\begin{assumption}[Overlap]\label{ass:overlap}
There exists $\varepsilon > 0$ such that for all $t$ and almost surely,
\begin{equation}\label{eq:overlap}
\varepsilon \le \pit \le 1 - \varepsilon.
\end{equation}
\end{assumption}

\begin{assumption}[Moment conditions]\label{ass:moments}
For each $a\in\{0,1\}$, $\E[|Y(a)|^4]<\infty$.
\end{assumption}

\begin{assumption}[Nuisance stability]\label{ass:nuis_stab}
There exists a finite constant $C<\infty$ such that for each $a\in\{0,1\}$,
\[
\sup_{n\ge 1}\ \sup_{t\le n}\E\big[|\widehat m_{t-1,a}(X_t)|^4\big] \le C.
\]
\end{assumption}

\begin{table}[t]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
Assumption & One-line meaning & Used in \\
\midrule
\ref{ass:iid} & i.i.d.\ arrivals of $(X_t,Y_t(0),Y_t(1))$ & Prop.~\ref{prop:identification}, Lem.~\ref{lem:mds}, Thm.~\ref{thm:studentized}, Lem.~\ref{lem:var_growth_suff} \\
\ref{ass:unconf} & sequential randomization with logged $\pi_t$ & Prop.~\ref{prop:identification}, Lem.~\ref{lem:mds}, Prop.~\ref{prop:var_decomp}, Thm.~\ref{thm:oracle_equiv}, Lem.~\ref{lem:var_growth_suff} \\
\ref{ass:overlap} & executed overlap $\pi_t\in[\varepsilon,1-\varepsilon]$ & Lem.~\ref{lem:moment_bounds}, Thm.~\ref{thm:studentized}, Thm.~\ref{thm:oracle_equiv}, Lem.~\ref{lem:var_growth_suff} \\
\ref{ass:moments} & $\E|Y(a)|^4<\infty$ & Lem.~\ref{lem:mds}, Lem.~\ref{lem:moment_bounds}, Thm.~\ref{thm:studentized} \\
\ref{ass:nuis_stab} & uniform 4th moments of $\widehat m_{t-1,a}(X_t)$ & Lem.~\ref{lem:mds}, Lem.~\ref{lem:moment_bounds} \\
\ref{ass:predictable_nuis} & nuisances use only past data ($\cF_{t-1}$); see Sec.~\ref{sec:crossfit} & Lem.~\ref{lem:mds}, Thm.~\ref{thm:studentized}, Thm.~\ref{thm:oracle_equiv} \\
\ref{ass:var_growth} & QV grows ($V_{\mathcal{T}}^2\gtrsim n_{\mathrm{eff}}$); see Sec.~\ref{sec:studentized_clt} & Thm.~\ref{thm:studentized} \\
\ref{ass:stab} & variance stabilizes ($V_{\mathcal{T}}^2/n_{\mathrm{eff}}\to V$); see Sec.~\ref{sec:variance_stab} & Cor.~\ref{cor:nonstudentized} \\
\ref{ass:l2} & $L^2$ nuisance consistency; see Sec.~\ref{sec:oracle_equiv} & Thm.~\ref{thm:oracle_equiv} \\
\ref{ass:noise} & nontrivial noise (primitive for var growth); see Sec.~\ref{sec:var_growth_sufficient} & Lem.~\ref{lem:var_growth_suff} \\
\bottomrule
\end{tabular}
\caption{Assumption ledger (referee-facing map). Some assumptions are introduced later; the ``see Sec.'' pointers indicate first appearance.}
\label{tab:assumption_ledger}
\end{table}

\begin{remark}[How to enforce Assumption~\ref{ass:nuis_stab}]\label{rem:enforce_nuis}
If outcomes have a known bounded range $Y\in[\underline y,\overline y]$ (or the analyst chooses conservative bounds), enforce nuisance stability by clipping predictions:
\[
\tilde m_{t-1,a}(x):=\clip\big(\widehat m_{t-1,a}(x),[\underline y,\overline y]\big),\qquad a\in\{0,1\},
\]
and use $\tilde m_{t-1}$ in \eqref{eq:phi_hat}. Since clipping is a deterministic map, if $\widehat m_{t-1}$ is $\cF_{t-1}$-measurable then so is $\tilde m_{t-1}$, hence predictability (Assumption~\ref{ass:predictable_nuis}) and the martingale property (Lemma~\ref{lem:mds}) are preserved.
\end{remark}

\begin{remark}[Minimal moments are possible but not needed for this note]\label{rem:min_moments}
Self-normalized martingale CLTs only require finite $(2+\delta)$ moments of the
score increments. We impose fourth moments to keep the quadratic-variation
equivalence proof fully transparent (a one-page Chebyshev argument).
\end{remark}

\begin{remark}[Assumptions in words and where they enter]\label{rem:assumptions}
Table~\ref{tab:assumption_ledger} summarizes each assumption and the results that invoke it. For validity of the studentized CI \eqref{eq:ci}, the essential design/analysis requirements are: logged propensities that equal the \emph{executed} randomization probabilities (Definition~\ref{def:propensity}); design-stage overlap (Assumption~\ref{ass:overlap}); predictable nuisance construction (Assumption~\ref{ass:predictable_nuis}); and moment/variance-growth conditions ensuring well-behaved score increments and growing quadratic variation (Assumptions~\ref{ass:moments}--\ref{ass:var_growth}). Asymptotics take $n_{\mathrm{eff}}\to\infty$ as $n\to\infty$.

\textbf{Design-stage requirement:} Assumption~\ref{ass:overlap} concerns the \emph{executed} probabilities used to draw $A_t$; post-hoc truncation of $1/\pi_t$ generally changes the estimand and requires separate bias analysis.
\end{remark}

% ============================================================================
% 3. PREDICTABLE AIPW/DML ESTIMATION
% ============================================================================
\section{Predictable AIPW/DML Estimation}\label{sec:score}

\subsection{The Doubly Robust Score}\label{sec:drscore}

Let $m_a^\star(x):=\E[Y(a)\mid X=x]$ denote the \emph{true} outcome regressions, and let $m=(m_0,m_1)$ denote generic measurable candidate regressions (potentially data-dependent through past information). The \emph{doubly robust} or \emph{AIPW} score for the ATE is
\begin{equation}\label{eq:psi_def}
\psi_t(\theta; m, \pi) = \frac{A_t(Y_t - m_1(X_t))}{\pit} - \frac{(1-A_t)(Y_t - m_0(X_t))}{1-\pit} + m_1(X_t) - m_0(X_t) - \theta.
\end{equation}

Define the \emph{pseudo-outcome} (score evaluated at $\theta = 0$):
\begin{equation}\label{eq:phi_def}
\phi_t(m) := \psi_t(0; m, \pi) = \frac{A_t(Y_t - m_1(X_t))}{\pit} - \frac{(1-A_t)(Y_t - m_0(X_t))}{1-\pit} + m_1(X_t) - m_0(X_t).
\end{equation}
Note that $\psi_t(\theta; m, \pi) = \phi_t(m) - \theta$.

\begin{remark}[Terminology: ``doubly robust'' vs.\ design-based unbiasedness]\label{rem:dr_terminology}
Because propensities are logged and assumed correct, $\phi_t(m)$ is conditionally
unbiased for $\tau(X_t)$ for \emph{any} predictable $m$ (Proposition~\ref{prop:identification}).
We nonetheless use the AIPW/DML terminology because the score is Neyman-orthogonal
and regression adjustment can substantially reduce variance (Proposition~\ref{prop:var_decomp}).
\end{remark}

\begin{remark}[Multi-arm extensions]\label{rem:multiarm_ptr}
Multi-arm versions follow componentwise; see Appendix~\ref{app:multiarm}.
\end{remark}

\subsection{Forward Cross-Fitting}\label{sec:crossfit}

Standard cross-fitting partitions data into folds and estimates nuisances on held-out folds. Under adaptivity, we must respect time: nuisance estimates used at time $t$ must be constructed from data \emph{strictly before} $t$.

\begin{definition}[Forward cross-fitting]\label{def:forward}
Partition indices $\{1, \ldots, n\}$ into $K$ contiguous blocks $I_1, \ldots, I_K$. The partition is \emph{deterministic} and fixed prior to data collection. For each block $k \ge 2$:
\begin{enumerate}[leftmargin=2em, label=\arabic*.]
\item Estimate nuisance functions $(\mhat^{(-k)}_0, \mhat^{(-k)}_1)$ using only data from blocks $I_1, \ldots, I_{k-1}$.
\item For each $t \in I_k$, evaluate the score using these estimates and the logged propensity $\pit$.
\end{enumerate}
For $t \in I_1$, use a pilot estimate or exclude $I_1$ from inference.
\end{definition}

\begin{remark}[Predictability is the only structural requirement]\label{rem:predictability_cf}
Forward cross-fitting is one convenient way to enforce predictability: for any
scored index $t$, the nuisance functions used in $\widehat\phi_t$ are
$\mathcal{F}_{t-1}$-measurable (i.e., constructed from past data only).
All results in Section~\ref{sec:main_results} apply to \emph{any} predictable
sequence $\{\widehat m_{t-1}\}$, including online updating rules that refit on
$\mathcal{F}_{t-1}$ each period.
\end{remark}

\begin{assumption}[Predictable nuisance construction]\label{ass:predictable_nuis}
For each $t$, the nuisance prediction functions used to score unit $t$,
$\widehat m_{t-1} = (\widehat m_{t-1,0},\widehat m_{t-1,1})$,
are $\mathcal{F}_{t-1}$-measurable.
Equivalently, conditional on $\mathcal{F}_{t-1}$, $\widehat m_{t-1,a}(\cdot)$ is a fixed (possibly random) function of past data, and does not depend on $(A_t,Y_t)$.
All tuning and model selection steps that affect $\widehat m_{t-1}$ (including any algorithmic randomness, e.g.\ randomized learners) are also $\mathcal{F}_{t-1}$-measurable; in practice, persist the training indices and the random seed per scored block (Section~\ref{sec:impl_protocol} and Appendix~\ref{app:repro}).
\end{assumption}

\begin{lemma}[A sufficient operational condition for predictability]\label{lem:operational_pred}
Under forward cross-fitting (Definition~\ref{def:forward}), if for each block
$I_k$ the analyst fits $(\widehat m_0^{(-k)},\widehat m_1^{(-k)})$ using only
data from blocks $I_1,\dots,I_{k-1}$ and then reuses these fitted objects
unchanged for all $t\in I_k$, then Assumption~\ref{ass:predictable_nuis} holds.
\end{lemma}

\begin{proof}
For $t\in I_k$, the fitted objects depend only on
$\sigma(Z_s: s\in I_1\cup\cdots\cup I_{k-1})\subseteq \mathcal{F}_{t-1}$ and are
therefore $\mathcal{F}_{t-1}$-measurable.
\end{proof}

\begin{remark}[Connection to A2IPW notation]\label{rem:a2ipw}
Our $\widehat{\phi}_t$ coincides with the A2IPW summand $h_t$ in the adaptive-experiment literature when
$\mhat_{t-1,a}$ plays the role of the outcome predictor $\widehat{f}_{t-1}(a,\cdot)$ used at time $t$.
\end{remark}

\subsection{The Estimator}\label{sec:estimator}

\paragraph{Scored index set and effective sample size.}
Let $\mathcal{T}\subseteq\{1,\dots,n\}$ denote the set of indices for which the nuisance used at time $t$ is predictable (i.e., $\widehat m_{t-1}$ is $\cF_{t-1}$-measurable).
Under forward cross-fitting with a burn-in block $I_1$, one typically takes $\mathcal{T}:=\{1,\dots,n\}\setminus I_1$.
Let $n_{\mathrm{eff}}:=\sum_{t=1}^n \1\{t\in\mathcal{T}\}=|\mathcal{T}|$.

\paragraph{Deterministic scoring set.}
Throughout the asymptotic theory we treat the scored index set $\mathcal{T}$ as
deterministic (fixed prior to data collection), as is the case under the
forward-block construction in Definition~\ref{def:forward}.

\begin{remark}[Scored set (deterministic in this note)]\label{rem:T_det}
Under forward cross-fitting with a pre-specified burn-in block $I_1$, the scored set $\mathcal{T}=\{1,\dots,n\}\setminus I_1$ is fixed prior to data collection, hence deterministic. This is the setting covered by Theorem~\ref{thm:studentized}. If instead $T_t:=\1\{t\in\mathcal{T}\}$ is allowed to be data-dependent but $\cF_{t-1}$-measurable (predictable), the same proof strategy applies to $\tilde\xi_t:=T_t(\widehat\phi_t-\theta_0)$; we omit this extension for brevity.
\end{remark}

\paragraph{Estimator and studentizing factor.}
The cross-fitted pseudo-outcome is
\begin{equation}\label{eq:phi_hat}
\widehat{\phi}_t = \phi_t(\widehat m_{t-1}),
\end{equation}
where $\widehat m_{t-1}$ is the predictable nuisance estimate.
Define
\begin{equation}\label{eq:theta_hat}
\widehat{\theta} := \frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}} \widehat{\phi}_t,
\end{equation}
\begin{equation}\label{eq:Vhat_def}
\widehat V := \frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}}(\widehat{\phi}_t-\widehat{\theta})^2.
\end{equation}
\begin{remark}[Studentizer and quadratic variation]\label{rem:studentizer_qv}
The quantity $\widehat V$ in \eqref{eq:Vhat_def} is a \emph{variance} (not a standard deviation). Writing $\xi_t:=\widehat\phi_t-\theta_0$, the realized quadratic variation is $Q_{\mathcal{T}}=\sum_{t\in\mathcal{T}}\xi_t^2$, while
\[
n_{\mathrm{eff}}\widehat V=\sum_{t\in\mathcal{T}}(\widehat\phi_t-\widehat\theta)^2
=Q_{\mathcal{T}}-n_{\mathrm{eff}}(\widehat\theta-\theta_0)^2.
\]
Thus $n_{\mathrm{eff}}\widehat V/Q_{\mathcal{T}}\pto 1$ under Theorem~\ref{thm:studentized}. Using the Bessel correction $1/(n_{\mathrm{eff}}-1)$ changes \eqref{eq:studentized_clt} only by a multiplicative $1+o_{\Prob}(1)$ factor.
\end{remark}
\paragraph{Score sum notation.}
Define the centered increments $\xi_t:=\widehat\phi_t-\theta_0$ and the score sum
$S_{\mathcal{T}}:=\sum_{t\in\mathcal{T}}\xi_t=n_{\mathrm{eff}}(\widehat\theta-\theta_0)$.

The Wald interval is
\begin{equation}\label{eq:ci}
\mathrm{CI}_{1-\alpha} = \Big[\widehat{\theta}\pm z_{1-\alpha/2}\sqrt{\widehat V/n_{\mathrm{eff}}}\Big].
\end{equation}



% ============================================================================
% 4. MAIN THEORETICAL RESULTS
% ============================================================================
\section{Main Theoretical Results}\label{sec:main_results}

\begin{remark}[Triangular-array and uniformity convention]\label{rem:triangular_uniform}
All objects may depend on the horizon $n$ (e.g.\ forward blocks and nuisance
fits). We suppress the index $n$ and assume overlap and moment constants are
uniform over $n$.
\end{remark}

This section presents our main theorems on the asymptotic behavior of the adaptive DML estimator. We include complete proofs in the main text to facilitate verification.

\subsection{The Martingale Structure}\label{sec:martingale}

The central insight of our analysis is that forward cross-fitting induces a martingale difference structure. We first establish an identification result for the pseudo-outcome.

\begin{proposition}[Identification (including predictable random nuisances)]\label{prop:identification}
Let $m_{t-1}=(m_{t-1,0},m_{t-1,1})$ be any (possibly random) pair of regression functions such that $m_{t-1}$ is $\cF_{t-1}$-measurable.
Under Assumptions~\ref{ass:iid} and~\ref{ass:unconf}, for each $t$,
\begin{equation}\label{eq:phi_ident}
\E\!\big[\phi_t(m_{t-1})\mid \cG_t\big]=\tau(X_t).
\end{equation}
Consequently, $\E[\phi_t(m_{t-1})]=\theta_0$.
\end{proposition}

\begin{proof}
Fix $t$ and condition on $\cG_t=\sigma(\cF_{t-1},X_t,\pi_t)$. Since $m_{t-1}$ is
$\cF_{t-1}$-measurable and $\cF_{t-1}\subseteq \cG_t$, the function $m_{t-1}$
is fixed under this conditioning.

By sequential randomization (Assumption~\ref{ass:unconf}),
\[
\E[Y_t \mid \cG_t, A_t=1]=\E[Y_t(1)\mid \cG_t],\qquad
\E[Y_t \mid \cG_t, A_t=0]=\E[Y_t(0)\mid \cG_t].
\]
By i.i.d.\ arrivals (Assumption~\ref{ass:iid}), $(Y_t(0),Y_t(1))\indep \cF_{t-1}$
and hence $\E[Y_t(a)\mid \cG_t]=\E[Y_t(a)\mid X_t]=m_a^\star(X_t)$ for $a\in\{0,1\}$.

Using $\E[A_t\mid \cG_t]=\pi_t$ and the tower property,
\begin{align*}
\E\!\left[\frac{A_t(Y_t-m_{t-1,1}(X_t))}{\pi_t}\,\Big|\,\cG_t\right]
&=
\E\!\left[\frac{A_t}{\pi_t}\,\E[Y_t-m_{t-1,1}(X_t)\mid \cG_t,A_t]\,\Big|\,\cG_t\right]\\
&=
\E\!\left[\frac{A_t}{\pi_t}\,(m_1^\star(X_t)-m_{t-1,1}(X_t))\,\Big|\,\cG_t\right]\\
&= m_1^\star(X_t)-m_{t-1,1}(X_t),
\end{align*}
and similarly
\[
\E\!\left[\frac{(1-A_t)(Y_t-m_{t-1,0}(X_t))}{1-\pi_t}\,\Big|\,\cG_t\right]
= m_0^\star(X_t)-m_{t-1,0}(X_t).
\]
Plugging into the definition of $\phi_t(m_{t-1})$ in \eqref{eq:phi_def} yields
\[
\E[\phi_t(m_{t-1})\mid \cG_t]
=
(m_{t-1,1}(X_t)-m_{t-1,0}(X_t))
+(m_1^\star(X_t)-m_{t-1,1}(X_t))
-(m_0^\star(X_t)-m_{t-1,0}(X_t))
=
m_1^\star(X_t)-m_0^\star(X_t)
=\tau(X_t),
\]
which proves \eqref{eq:phi_ident}. Taking unconditional expectations gives
$\E[\phi_t(m_{t-1})]=\E[\tau(X_t)]=\theta_0$.
\end{proof}

The key consequence is that the pseudo-outcome, when evaluated with \emph{any} predictable nuisance estimates, remains conditionally unbiased.

\begin{lemma}[Martingale difference structure]\label{lem:mds}
Suppose Assumptions~\ref{ass:iid}, \ref{ass:unconf}, \ref{ass:moments}, and \ref{ass:nuis_stab} hold. Let $\{\mhat_{t-1}\}$ satisfy Assumption~\ref{ass:predictable_nuis} (equivalently, $\mhat_{t-1}$ is $\cF_{t-1}$-measurable for each $t$). Then:
\begin{equation}\label{eq:mds}
\E[\phi_t(\mhat_{t-1}) \given \cF_{t-1}] = \theta_0,
\end{equation}
and the centered sequence $\xi_t := \phi_t(\mhat_{t-1}) - \theta_0$ is a martingale difference sequence with respect to $\{\cF_t\}$.
\end{lemma}

\begin{proof}
(Integrability is ensured by Assumptions~\ref{ass:moments}--\ref{ass:nuis_stab}.)
Using iterated expectations and Proposition~\ref{prop:identification},
\[
\E[\phi_t(\widehat m_{t-1})\mid \cF_{t-1}]
=
\E\!\big[\E[\phi_t(\widehat m_{t-1})\mid \cG_t]\mid \cF_{t-1}\big]
=
\E[\tau(X_t)\mid \cF_{t-1}].
\]
By Assumption~\ref{ass:iid}, $X_t$ is independent of $\cF_{t-1}$, so $\E[\tau(X_t) \given \cF_{t-1}] = \E[\tau(X)] = \theta_0$.
\end{proof}

\begin{corollary}[Finite-sample unbiasedness]\label{cor:unbiased}
Under Assumptions~\ref{ass:iid}, \ref{ass:unconf}, \ref{ass:moments}, \ref{ass:nuis_stab}, and predictable nuisances as in Assumption~\ref{ass:predictable_nuis}, we have $\E[\thetahat]=\theta_0$.
\end{corollary}

\begin{proof}
By linearity and Lemma~\ref{lem:mds},
\[
\E[\widehat\theta]
=
\frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}}
\E[\phi_t(\widehat m_{t-1})]
=
\frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}} \theta_0
=
\theta_0.
\]
\end{proof}

\begin{remark}[Interpretation]\label{rem:mds_interpretation}
Lemma~\ref{lem:mds} is the conceptual replacement for fold-wise independence in i.i.d.\ DML. Rather than requiring approximate independence between nuisance estimation and score evaluation, we exploit exact conditional unbiasedness given the past. This martingale structure holds for \emph{any} quality of nuisance estimates, provided they are predictable.
\end{remark}

\begin{lemma}[Uniform fourth-moment bounds for the score increments]\label{lem:moment_bounds}
Assume Assumptions~\ref{ass:overlap}, \ref{ass:moments}, and \ref{ass:nuis_stab}.
Let $\widehat{\phi}_t=\phi_t(\widehat m_{t-1})$ with $\widehat m_{t-1}$ satisfying Assumption~\ref{ass:predictable_nuis}. Then there exists $C<\infty$ such that
\[
\sup_{n\ge 1}\ \sup_{t\le n}\E\big[|\widehat{\phi}_t|^4\big]\le C,\qquad
\sup_{n\ge 1}\ \sup_{t\le n}\E\big[|\xi_t|^4\big]\le C,\qquad
\sup_{n\ge 1}\ \sup_{t\le n}\E\big[\xi_t^2\big]\le C,
\]
where $\xi_t:=\widehat{\phi}_t-\theta_0$.
\end{lemma}

\begin{proof}
By overlap, $\pi_t^{-1}\le \varepsilon^{-1}$ and $(1-\pi_t)^{-1}\le \varepsilon^{-1}$ a.s.
Write $\widehat{\phi}_t$ as a sum of four terms and use
$(u_1+\cdots+u_4)^4\le 4^3\sum_{j=1}^4 u_j^4$ and $A_t\in\{0,1\}$ to obtain
\[
|\widehat{\phi}_t|^4 \le C_1\Big(|Y_t|^4+|\widehat m_{t-1,1}(X_t)|^4+|\widehat m_{t-1,0}(X_t)|^4\Big)
\]
for $C_1=C_1(\varepsilon)$. Since $|Y_t|^4\le |Y_t(1)|^4+|Y_t(0)|^4$, the bound
follows from Assumptions~\ref{ass:moments}--\ref{ass:nuis_stab}.
The $\xi_t$ bounds follow from $(u-v)^4\le 8(u^4+v^4)$ and Cauchy--Schwarz.
\end{proof}

\begin{remark}[Weaker than i.i.d.]\label{rem:weaker_iid}
Lemma~\ref{lem:mds} uses Assumption~\ref{ass:iid} only through the implication $\E[\tau(X_t)\mid \cF_{t-1}]=\theta_0$.
Thus the MDS argument extends to any arrival process satisfying this mean-stationarity condition.
\end{remark}

\subsection{The Variance Structure}\label{sec:variance_structure}

Before stating the CLT, we analyze the variance structure. Define the \emph{realized} and \emph{predictable} quadratic variations:
\begin{equation}\label{eq:quadratic_variations}
Q_{\mathcal{T}} := \sum_{t\in\mathcal{T}} \xi_t^2, \qquad
V_{\mathcal{T}}^2 := \sum_{t\in\mathcal{T}} \E[\xi_t^2 \given \cF_{t-1}].
\end{equation}

\begin{proposition}[Conditional second moment for general $m$ (variance decomposition)]\label{prop:var_decomp}
Let $m=(m_0,m_1)$ be any measurable functions, and let
$m_a^\star(x)=\E[Y(a)\mid X=x]$ denote the true regressions. Define
$b_a(x):=m_a^\star(x)-m_a(x)$ and $\sigma_a^2(x):=\Var(Y(a)\mid X=x)$.
Under Assumptions~\ref{ass:iid} and~\ref{ass:unconf}, for each $t$,
\begin{align}
\E\!\big[(\phi_t(m)-\theta_0)^2 \given \cG_t\big]
&=
\frac{\sigma_1^2(X_t)}{\pit}
+\frac{\sigma_0^2(X_t)}{1-\pit}
+(\tau(X_t)-\theta_0)^2 \nonumber\\
&\quad
+\pit(1-\pit)\left(\frac{b_1(X_t)}{\pit}+\frac{b_0(X_t)}{1-\pit}\right)^2.
\label{eq:general_second_moment}
\end{align}
In particular, the conditional second moment is minimized at $m=m^\star$, and
the nuisance-error term is nonnegative.
\end{proposition}

\begin{proof}
Since $\cG_t$ contains $\cF_{t-1}$, when $m=\widehat m_{t-1}$ is predictable we
may condition on $\cG_t$ and treat $m$ as fixed in the algebra below.
Fix $t$ and condition on $\cG_t$. Let $\varepsilon_{t,a}:=Y_t(a)-m_a^\star(X_t)$ so that
$\E[\varepsilon_{t,a}\mid X_t]=0$ and $\E[\varepsilon_{t,a}^2\mid X_t]=\sigma_a^2(X_t)$.
A direct algebraic expansion gives
\[
\phi_t(m)-\tau(X_t)
=
\frac{A_t}{\pit}\varepsilon_{t,1}
-\frac{1-A_t}{1-\pit}\varepsilon_{t,0}
+(A_t-\pit)\left(\frac{b_1(X_t)}{\pit}+\frac{b_0(X_t)}{1-\pit}\right).
\]
Under Assumption~\ref{ass:unconf}, $(A_t-\pit)$ is mean-zero given $\cG_t$ and independent of $(\varepsilon_{t,0},\varepsilon_{t,1})$ conditional on $X_t$.
Moreover, $A_t(1-A_t)=0$ ensures the treated/control residual terms do not cross.
Taking conditional second moments yields \eqref{eq:general_second_moment}, and adding $(\tau(X_t)-\theta_0)^2$ completes the result.
\end{proof}

\begin{remark}[Oracle case and variance inflation are immediate]\label{rem:oracle_from_decomp}
Setting $m=m^\star$ in Proposition~\ref{prop:var_decomp} removes the
nonnegative augmentation term and yields the oracle conditional second moment.
The factors $1/\pi_t$ and $1/(1-\pi_t)$ make explicit how extreme propensities
inflate uncertainty, motivating overlap enforcement at the \emph{design} stage.
\end{remark}

\begin{remark}[Why learn $m$ if validity does not require consistency?]\label{rem:why_learn_m}
Proposition~\ref{prop:var_decomp} shows that using $m=m^\star$ minimizes the conditional second moment of the pseudo-outcome, and any misspecification (or noisy learning) can only \emph{inflate} variance through a nonnegative augmentation term.
Thus, predictability delivers validity \emph{without} nuisance-rate conditions (Theorem~\ref{thm:studentized}), while accurate learning improves precision by reducing $\Vhat$.
The IPW estimator corresponds to $m_0\equiv m_1\equiv 0$ and typically has larger variance.
\end{remark}

\subsection{Studentized Asymptotic Normality}\label{sec:studentized_clt}

We now state our main inferential result. The key ingredient is that the
realized quadratic variation $Q_{\mathcal{T}}$ and its predictable counterpart
$V_{\mathcal{T}}^2$ are asymptotically equivalent under mild conditions,
enabling studentization with the feasible sample variance $\widehat V$.

\paragraph{Roadmap.}
Lemma~\ref{lem:mds} gives an MDS for $\xi_t=\widehat\phi_t-\theta_0$, and
Lemma~\ref{lem:moment_bounds} plus Assumption~\ref{ass:var_growth} verify the
conditions of the self-normalized martingale CLT stated as Theorem~\ref{thm:selfnorm}
in Appendix~\ref{app:mlt}. Feasibility follows from $n_{\mathrm{eff}}\widehat V/Q_{\mathcal{T}}\pto 1$.

\begin{assumption}[Variance growth / nondegeneracy]\label{ass:var_growth}
As $n\to\infty$, $n_{\mathrm{eff}}:=|\mathcal{T}|\to\infty$ and there exists
$v_->0$ such that
\[
\Prob\!\big(V_{\mathcal{T}}^2 \ge v_-\, n_{\mathrm{eff}}\big) \to 1.
\]
\end{assumption}

A simple primitive sufficient condition is given in
Section~\ref{sec:var_growth_sufficient} (nontrivial noise + overlap).

\begin{remark}[Nondegeneracy and numerical guardrails]\label{rem:Vhat_pos}
Assumption~\ref{ass:var_growth} implies the quadratic variation grows, so
$\widehat V$ is bounded away from zero with high probability asymptotically.
In finite samples, if $\widehat V=0$ (e.g.\ constant outcomes), inference is
uninformative; report this as a design/measurement degeneracy.
\end{remark}

\begin{theorem}[Studentized martingale CLT]\label{thm:studentized}
Suppose Assumptions~\ref{ass:iid}--\ref{ass:var_growth} hold and
Assumption~\ref{ass:predictable_nuis} holds. Assume the scored set $\mathcal{T}$
is deterministic (e.g.\ induced by forward blocks).
Let $\xi_t:=\widehat\phi_t-\theta_0$ for $t\in\mathcal{T}$.
Then
\begin{equation}\label{eq:studentized_clt}
\frac{\sqrt{n_{\mathrm{eff}}}(\widehat\theta-\theta_0)}{\sqrt{\widehat V}}
\dto \mathcal{N}(0,1).
\end{equation}
Consequently, $\Prob(\theta_0 \in \mathrm{CI}_{1-\alpha}) \to 1 - \alpha$.
\end{theorem}

\begin{proof}
The proof proceeds in two steps.

\medskip\noindent\emph{Indexing convention.}
To match Theorem~\ref{thm:selfnorm} (stated for sums over $t=1,\dots,n$), define the extended increments
\[
\tilde\xi_t := T_t(\widehat\phi_t-\theta_0), \qquad T_t:=\1\{t\in\mathcal{T}\},
\qquad t=1,\dots,n.
\]
Since $\mathcal{T}$ is deterministic, $T_t$ is nonrandom and
$\E[\tilde\xi_t\mid\mathcal{F}_{t-1}]=T_t\E[\widehat\phi_t-\theta_0\mid\mathcal{F}_{t-1}]=0$,
so $\{\tilde\xi_t,\mathcal{F}_t\}$ is a martingale difference array.
$\sum_{t=1}^n \tilde\xi_t = S_{\mathcal{T}}$, $\sum_{t=1}^n \tilde\xi_t^{\,2} = Q_{\mathcal{T}}$,
and $\sum_{t=1}^n \E[\tilde\xi_t^{\,2}\mid \mathcal{F}_{t-1}] = V_{\mathcal{T}}^2$.
For readability we drop the tilde notation below.

\medskip\noindent\textbf{Step 1: Self-normalized CLT for $S_{\mathcal{T}}/\sqrt{Q_{\mathcal{T}}}$.}
Let $S_{\mathcal{T}} = \sum_{t\in\mathcal{T}} \xi_t = n_{\mathrm{eff}}(\widehat\theta - \theta_0)$. We apply Theorem~\ref{thm:selfnorm} (stated in Appendix~\ref{app:mlt}). Assumption~\ref{ass:var_growth} implies $V_{\mathcal{T}}^2 \to \infty$ in probability.

To show $Q_{\mathcal{T}}/V_{\mathcal{T}}^2\pto 1$, define the martingale differences
$\Delta_t:=\xi_t^2-\E[\xi_t^2\mid \mathcal{F}_{t-1}]$ and the partial sums
$M_k:=\sum_{t=1}^k\Delta_t$ for $k=1,\dots,n$. Then $\{M_k,\mathcal{F}_k\}$ is a martingale and
$M_n=Q_{\mathcal{T}}-V_{\mathcal{T}}^2$.
By Lemma~\ref{lem:moment_bounds}, $\sup_t\E[\xi_t^4]<\infty$, and Jensen implies
$\E[\E[\xi_t^2\mid\mathcal{F}_{t-1}]^2]\le \E[\xi_t^4]$, hence
$\sup_t\E[\Delta_t^2]\le 4\sup_t\E[\xi_t^4]<\infty$.
By orthogonality of martingale differences,
\[
\E[M_n^2]=\sum_{t\in\mathcal{T}}\E[\Delta_t^2]=O(n_{\mathrm{eff}}).
\]
Let $\mathcal{E}_{\mathcal{T}}:=\{V_{\mathcal{T}}^2\ge v_- n_{\mathrm{eff}}\}$,
which satisfies $\Prob(\mathcal{E}_{\mathcal{T}})\to 1$ by Assumption~\ref{ass:var_growth}.
On $\mathcal{E}_{\mathcal{T}}$,
\[
\left|\frac{Q_{\mathcal{T}}}{V_{\mathcal{T}}^2}-1\right|
\le \frac{|M_n|}{v_- n_{\mathrm{eff}}}.
\]
Therefore, for any $\eta>0$,
\[
\Prob\!\left(\left|\frac{Q_{\mathcal{T}}}{V_{\mathcal{T}}^2}-1\right|>\eta\right)
\le \Prob(\mathcal{E}_{\mathcal{T}}^c)
+\frac{\E[M_n^2]}{\eta^2 v_-^2 n_{\mathrm{eff}}^{2}}
\to 0,
\]
so $Q_{\mathcal{T}}/V_{\mathcal{T}}^2\pto 1$.

For the Lyapunov condition with $\delta=2$, Lemma~\ref{lem:moment_bounds} gives
$\sup_t\E|\xi_t|^4<\infty$ and on $\mathcal{E}_{\mathcal{T}}$,
\[
\frac{\sum_{t\in\mathcal{T}}\E|\xi_t|^4}{V_{\mathcal{T}}^4}
\le \frac{C n_{\mathrm{eff}}}{(v_- n_{\mathrm{eff}})^2}\to 0,
\]
hence condition (iii) of Theorem~\ref{thm:selfnorm} holds.
By Theorem~\ref{thm:selfnorm}, $S_{\mathcal{T}}/\sqrt{Q_{\mathcal{T}}} \dto \mathcal{N}(0,1)$.

\medskip\noindent\textbf{Step 2: Replace $Q_{\mathcal{T}}$ with $n_{\mathrm{eff}}\widehat{V}$.}
We have
\[
n_{\mathrm{eff}}\widehat V = \sum_{t\in\mathcal{T}} (\widehat{\phi}_t - \widehat\theta)^2 = \sum_{t\in\mathcal{T}} (\xi_t - (\widehat\theta - \theta_0))^2 = Q_{\mathcal{T}} - n_{\mathrm{eff}}(\widehat\theta - \theta_0)^2.
\]
Since $S_{\mathcal{T}}/\sqrt{Q_{\mathcal{T}}} = O_\Prob(1)$, we have $S_{\mathcal{T}}^2/Q_{\mathcal{T}} = O_\Prob(1)$, hence
\[
\frac{n_{\mathrm{eff}}(\widehat\theta - \theta_0)^2}{Q_{\mathcal{T}}} = \frac{S_{\mathcal{T}}^2}{n_{\mathrm{eff}} Q_{\mathcal{T}}} = \frac{1}{n_{\mathrm{eff}}} \cdot \frac{S_{\mathcal{T}}^2}{Q_{\mathcal{T}}} = o_\Prob(1).
\]
Thus $n_{\mathrm{eff}}\widehat V/Q_{\mathcal{T}} \pto 1$, and by Slutsky's lemma:
\[
\frac{S_{\mathcal{T}}}{\sqrt{n_{\mathrm{eff}}\widehat V}} = \frac{S_{\mathcal{T}}}{\sqrt{Q_{\mathcal{T}}}} \cdot \sqrt{\frac{Q_{\mathcal{T}}}{n_{\mathrm{eff}}\widehat V}} \dto \mathcal{N}(0,1). \qedhere
\]
\end{proof}

\begin{remark}[Validity vs.\ precision (and consistency)]\label{rem:no_rates}
The studentized CLT in Theorem~\ref{thm:studentized} does not require
$\widehat m_{t-1}$ to converge to $m^\star$ for \emph{validity}; predictability
(Assumption~\ref{ass:predictable_nuis}) and moment/nondegeneracy conditions are
sufficient. Nuisance learning is nonetheless valuable: by
Proposition~\ref{prop:var_decomp} it can only reduce the conditional variance of
the score and tighten $\mathrm{CI}_{1-\alpha}$. In particular, $\widehat\theta$
is consistent whenever $n_{\mathrm{eff}}\to\infty$, and $\widehat V$ is a feasible
studentizer even when $V_{\mathcal{T}}^2/n_{\mathrm{eff}}$ does not stabilize.
\end{remark}

\begin{remark}[Contrast with stabilization-based CLTs]\label{rem:contrast_stab}
Stabilization-based analyses derive a non-studentized CLT with deterministic
asymptotic variance, typically requiring $\pi_t\to\pi$ and nuisance consistency.
Theorem~\ref{thm:studentized} instead delivers a studentized CLT without
stabilization: $\widehat V$ adapts to the realized propensity/outcome path on
$\mathcal{T}$.
\end{remark}

\subsection{Variance Consistency Under Stabilization}\label{sec:variance_stab}

When the conditional variance stabilizes, we obtain a conventional CLT.

\begin{assumption}[Variance stabilization]\label{ass:stab}
There exists $V \in (0,\infty)$ such that $V_{\mathcal{T}}^2/n_{\mathrm{eff}} \pto V$.
\end{assumption}

\begin{corollary}[Non-studentized CLT]\label{cor:nonstudentized}
Assume Assumptions~\ref{ass:iid}--\ref{ass:stab} and Assumption~\ref{ass:predictable_nuis}. Assume also the scored set $\mathcal{T}$ is deterministic (as in Remark~\ref{rem:T_det}). Then
\[
\sqrt{n_{\mathrm{eff}}}(\thetahat - \theta_0) \dto \mathcal{N}(0, V), \qquad \Vhat \pto V.
\]
\end{corollary}

\begin{proof}
By Theorem~\ref{thm:studentized},
\[
\frac{\sqrt{n_{\mathrm{eff}}}(\widehat\theta-\theta_0)}{\sqrt{\widehat V}} \dto \mathcal{N}(0,1).
\]
Moreover the proof of Theorem~\ref{thm:studentized} shows both
$Q_{\mathcal{T}}/V_{\mathcal{T}}^2\pto 1$ and
$n_{\mathrm{eff}}\widehat V/Q_{\mathcal{T}}\pto 1$, hence
$\widehat V = (V_{\mathcal{T}}^2/n_{\mathrm{eff}})\cdot(1+o_{\Prob}(1))$.
Under Assumption~\ref{ass:stab}, $V_{\mathcal{T}}^2/n_{\mathrm{eff}}\pto V$,
so $\widehat V\pto V$ and Slutsky yields
$\sqrt{n_{\mathrm{eff}}}(\widehat\theta-\theta_0)\dto \mathcal{N}(0,V)$.
\end{proof}

\subsection{Oracle Equivalence}\label{sec:oracle_equiv}

Under additional nuisance consistency, the feasible estimator is asymptotically equivalent to the oracle.

\begin{assumption}[$L^2$ nuisance consistency]\label{ass:l2}
Let $m_a^\star(x):=\E[Y(a)\mid X=x]$.
Then
\[
\frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}}
\E\Big[
\big(\widehat m_{t-1,1}(X_t)-m_1^\star(X_t)\big)^2
+
\big(\widehat m_{t-1,0}(X_t)-m_0^\star(X_t)\big)^2
\Big]\to 0.
\]
\end{assumption}

\begin{theorem}[Oracle equivalence]\label{thm:oracle_equiv}
Assume Assumptions~\ref{ass:unconf}, \ref{ass:overlap}, \ref{ass:predictable_nuis}, and \ref{ass:l2}. Assume also the scored set $\mathcal{T}$ is deterministic (as in Remark~\ref{rem:T_det}). Let $m^\star:=(m_0^\star,m_1^\star)$ and define the oracle estimator
\[
\widehat{\theta}^\star := \frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}} \phi_t(m^\star).
\]
Then
\[
\sqrt{n_{\mathrm{eff}}}(\thetahat - \widehat{\theta}^\star) \pto 0.
\]
\end{theorem}

\begin{proof}
Let $e_{t-1,a}(x):=\widehat m_{t-1,a}(x)-m_a^\star(x)$.
A direct algebraic calculation gives, for each scored index $t\in\mathcal{T}$,
\[
\phi_t(\widehat m_{t-1})-\phi_t(m^\star)
=
-(A_t-\pi_t)\left(\frac{e_{t-1,1}(X_t)}{\pi_t}+\frac{e_{t-1,0}(X_t)}{1-\pi_t}\right)
=:\Delta_t.
\]
By predictability, $(e_{t-1,0},e_{t-1,1})$ is $\cF_{t-1}$-measurable, and $\pi_t$ is $\cG_t$-measurable.
Using iterated expectations,
\[
\E[\Delta_t\mid \cF_{t-1}]
=
\E\!\left[\E[\Delta_t\mid \cG_t]\mid \cF_{t-1}\right]
=
-\E\!\left[\left(\frac{e_{t-1,1}(X_t)}{\pi_t}+\frac{e_{t-1,0}(X_t)}{1-\pi_t}\right)
\E[A_t-\pi_t\mid \cG_t]\Bigm|\cF_{t-1}\right]
=0,
\]
so $\{\Delta_t,\cF_t\}$ is a martingale difference sequence over $t\in\mathcal{T}$.
Now
\[
\sqrt{n_{\mathrm{eff}}}\,(\widehat\theta-\widehat\theta^\star)
=
\frac{1}{\sqrt{n_{\mathrm{eff}}}}\sum_{t\in\mathcal{T}} \Delta_t.
\]
By orthogonality of martingale differences,
\[
\E\!\left[\Big(\sqrt{n_{\mathrm{eff}}}\,(\widehat\theta-\widehat\theta^\star)\Big)^2\right]
=
\frac{1}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}}\E[\Delta_t^2].
\]
Conditional on $\cG_t$, $\E[(A_t-\pi_t)^2\mid \cG_t]=\pi_t(1-\pi_t)$, hence
\[
\E[\Delta_t^2\mid \cG_t]
=
\pi_t(1-\pi_t)\left(\frac{e_{t-1,1}(X_t)}{\pi_t}+\frac{e_{t-1,0}(X_t)}{1-\pi_t}\right)^2
\le C\big(e_{t-1,1}(X_t)^2+e_{t-1,0}(X_t)^2\big)
\]
for a constant $C<\infty$ depending only on the overlap constant $\varepsilon$.
Taking expectations and averaging over $t\in\mathcal{T}$ yields
\[
\E\!\left[\Big(\sqrt{n_{\mathrm{eff}}}\,(\widehat\theta-\widehat\theta^\star)\Big)^2\right]
\le
\frac{C}{n_{\mathrm{eff}}}\sum_{t\in\mathcal{T}}
\E\big[e_{t-1,1}(X_t)^2+e_{t-1,0}(X_t)^2\big]
\to 0
\]
by Assumption~\ref{ass:l2}. Therefore $\sqrt{n_{\mathrm{eff}}}(\widehat\theta-\widehat\theta^\star)\pto 0$.
\end{proof}

\begin{remark}[Relation to i.i.d.\ DML rate conditions]\label{rem:n14}
In i.i.d.\ DML analyses of the ATE with \emph{unknown} propensities, asymptotic linearity typically requires a product-rate condition between the propensity-score error and the outcome-regression error (often enforced via $o_P(n^{-1/4})$-type rates for each component).
In our setting, propensities are assumed known and logged, so the remaining nuisance component enters only through predictable regression adjustments.
As a result, nuisance convergence is not needed for validity of the studentized CLT, and $L^2$ consistency is sufficient for oracle equivalence (Theorem~\ref{thm:oracle_equiv}).
\end{remark}

\subsection{A Sufficient Condition for Variance Growth}\label{sec:var_growth_sufficient}

Assumption~\ref{ass:var_growth} is mild but abstract. The following provides a primitive sufficient condition.

\begin{assumption}[Nontrivial noise]\label{ass:noise}
$\E[\sigma_1^2(X) + \sigma_0^2(X)] \ge \underline{\sigma}^2 > 0$.
\end{assumption}

\begin{lemma}[A sufficient condition for variance growth]\label{lem:var_growth_suff}
Assume Assumptions~\ref{ass:iid}, \ref{ass:unconf}, \ref{ass:overlap}, and \ref{ass:noise}.
Then there exists $v_->0$ such that for every scored set $\mathcal{T}$ with
$n_{\mathrm{eff}}:=|\mathcal{T}|\to\infty$,
\[
\Prob\!\big(V_{\mathcal{T}}^2 \ge v_-\, n_{\mathrm{eff}}\big)\to 1,
\]
so Assumption~\ref{ass:var_growth} holds.
\end{lemma}

\begin{proof}
Fix $t\in\mathcal{T}$. By Proposition~\ref{prop:var_decomp} and nonnegativity of
the augmentation term,
\[
\E[\xi_t^2\mid \cG_t]
\ge
\frac{\sigma_1^2(X_t)}{\pit}+\frac{\sigma_0^2(X_t)}{1-\pit}.
\]
By overlap (Assumption~\ref{ass:overlap}), $\pi_t\le 1-\varepsilon$ and $1-\pi_t\le 1-\varepsilon$, hence
\[
\frac{\sigma_1^2(X_t)}{\pi_t}+\frac{\sigma_0^2(X_t)}{1-\pi_t}
\ge
\frac{\sigma_1^2(X_t)+\sigma_0^2(X_t)}{1-\varepsilon}.
\]
Taking $\E[\cdot\mid \cF_{t-1}]$ and using i.i.d.\ arrivals (Assumption~\ref{ass:iid}) yields
\[
\E[\xi_t^2\mid \cF_{t-1}]
=
\E\big[\E[\xi_t^2\mid \cG_t]\mid \cF_{t-1}\big]
\ge
\frac{1}{1-\varepsilon}\E[\sigma_1^2(X)+\sigma_0^2(X)]
\ge
\frac{\underline{\sigma}^2}{1-\varepsilon}
=:v_-,
\]
almost surely.
Summing over $t\in\mathcal{T}$ gives $V_{\mathcal{T}}^2=\sum_{t\in\mathcal{T}}\E[\xi_t^2\mid \cF_{t-1}]\ge v_- n_{\mathrm{eff}}$ almost surely, hence the probability statement holds.
\end{proof}

% ============================================================================
% 5. IMPLEMENTATION
% ============================================================================
\section{Implementation}\label{sec:implementation}
\subsection{Algorithm}\label{sec:algorithm}
\paragraph{Inputs.}
A time-ordered log $(X_t,A_t,Y_t,\pi_t)_{t=1}^n$ where $\pi_t$ is the executed
assignment probability used to randomize $A_t$.

\paragraph{Checklist (must hold for Theorem~\ref{thm:studentized}).}
\begin{enumerate}[leftmargin=2em,label=(C\arabic*)]
\item \textbf{Executed overlap:} enforce $\pi_t\in[\varepsilon,1-\varepsilon]$ \emph{before} drawing $A_t$; the logged $\pi_t$ must match the executed value.
\item \textbf{Predictable nuisances:} for each scored $t$, the nuisance object $\widehat m_{t-1}$ used in $\widehat\phi_t$ is $\mathcal{F}_{t-1}$-measurable and frozen within scored blocks (Sec.~\ref{sec:crossfit}).
\item \textbf{Moment guardrails:} when appropriate, clip predictions to a conservative outcome range to enforce Assumption~\ref{ass:nuis_stab} (Remark~\ref{rem:enforce_nuis}).
\item \textbf{Scored set:} use a scored set $\mathcal{T}$ fixed ex ante (this note) with $n_{\mathrm{eff}}\to\infty$; report the burn-in rule explicitly.
\end{enumerate}

\paragraph{Estimator.}
Compute $\widehat{\phi}_t$ as in \eqref{eq:phi_hat} using the score \eqref{eq:phi_def} with a predictable nuisance $\widehat m_{t-1}$ (Assumption~\ref{ass:predictable_nuis}). Form $(\widehat\theta,\widehat V)$ as in \eqref{eq:theta_hat}--\eqref{eq:Vhat_def} on the scored set $\mathcal{T}$, and report the Wald interval \eqref{eq:ci}.

\subsection{Reference Implementation and Audit Protocol}\label{sec:impl_protocol}
For referee-facing reproducibility, enforce the logging contract in
Appendix~\ref{app:repro} and persist (per scored block) the training indices and
the \emph{frozen} nuisance object used to score that block; this is sufficient to
audit Assumption~\ref{ass:predictable_nuis} under forward cross-fitting.

\subsection{Practical Guidance}\label{sec:guidance}
\paragraph{Diagnostics to report.}
\begin{enumerate}[leftmargin=2em,label=(D\arabic*)]
\item Propensity path summary on $\mathcal{T}$: min, 5\%, 25\%, 50\%, 75\%, 95\%, max of $\{\pi_t\}$ and the executed threshold $\varepsilon$.
\item Per-arm counts: $\sum_{t\in\mathcal{T}}A_t$ and $\sum_{t\in\mathcal{T}}(1-A_t)$.
\item Weight extremeness: summary of $\{1/(\pi_t(1-\pi_t))\}$ on $\mathcal{T}$ (mean and 95th percentile).
\item Sensitivity to the forward-block partition (e.g.\ $K$ and burn-in size); do not apply post-hoc truncation beyond the executed overlap rule.
\end{enumerate}

% ============================================================================
% 6. SIMULATION STUDY
% ============================================================================
\section{Simulation Study: Regime-Switching Propensities}\label{sec:simulation}

\paragraph{Design.}
We simulate a regime-switching adaptive assignment with logged propensities. For
$t\le n_0=50$ (burn-in), set $\pi_t=0.5$. Let $\widehat\tau_{\mathrm{burn}}$ be the
burn-in difference-in-means and set $\pi_t=0.8$ for $t>n_0$ if
$\widehat\tau_{\mathrm{burn}}\ge 0$, and $\pi_t=0.2$ otherwise. Outcomes satisfy
$Y(0)\sim\mathcal{N}(0,1)$ and $Y(1)=\tau+\varepsilon_1$ with $\tau=0$ and
$\varepsilon_1\sim\mathcal{N}(0,9)$ (no covariates, so AIPW reduces to IPW with logged $\pi_t$).
We take $n\in\{500,2000\}$ and $R=3000$ replications, comparing (i) \emph{SN}, the
self-normalized interval \eqref{eq:ci}; and (ii) \emph{Fixed}, a misspecified
fixed-variance interval computed as if propensities stabilized at $0.5$.

\begin{table}[ht]
\centering
\small
\begin{tabular}{rllrrr}
\toprule
$n$ & method & regime & coverage & avg. length & count \\
\midrule
500 & Fixed & high & 0.965 & 0.784 & 1525 \\
500 & Fixed & low & 0.812 & 0.784 & 1475 \\
500 & Fixed & all & 0.890 & 0.784 & 3000 \\
500 & SN & high & 0.942 & 0.714 & 1525 \\
500 & SN & low & 0.942 & 1.155 & 1475 \\
500 & SN & all & 0.942 & 0.931 & 3000 \\
2000 & Fixed & high & 0.971 & 0.392 & 1492 \\
2000 & Fixed & low & 0.810 & 0.392 & 1508 \\
2000 & Fixed & all & 0.890 & 0.392 & 3000 \\
2000 & SN & high & 0.951 & 0.354 & 1492 \\
2000 & SN & low & 0.950 & 0.592 & 1508 \\
2000 & SN & all & 0.950 & 0.474 & 3000 \\
\bottomrule
\end{tabular}
\caption{Monte Carlo coverage (two-sided 95\% Wald intervals) by regime for the regime-switching propensity design. Rows labeled ``all'' report unconditional performance aggregated across regimes (weighted by the regime counts).}
\label{tab:sim_regime_conditional}
\end{table}

\paragraph{Interpretation.}
Because the post-burn-in propensity is random, $V_{\mathcal{T}}^2/n_{\mathrm{eff}}$
is random across replications, violating Assumption~\ref{ass:stab} behind fixed-variance
Wald approximations. The Fixed method under-covers in the low-propensity regime,
while SN adapts to realized quadratic variation and delivers near-nominal coverage
in both regimes (with longer intervals when propensities are more extreme).

% ============================================================================
% 7. CONCLUSION
% ============================================================================
\section{Conclusion}\label{sec:conclusion}
In adaptive experiments with logged predictable propensities, predictable
construction of nuisance fits converts the orthogonal AIPW/DML score increments
into an exact martingale difference array. Studentization by realized quadratic
variation yields fixed-horizon Wald inference without requiring propensity
stabilization or nuisance convergence rates for validity. Optional add-ons give
deterministic-variance asymptotics under stabilization and oracle equivalence
under $L^2$ nuisance consistency.

% ============================================================================
% APPENDIX
% ============================================================================
\appendix

\section{Martingale Limit Theory}\label{app:mlt}
We use standard martingale CLTs; see \citet{hall2014martingale} and
\citet{delapena2009self}.

\medskip\noindent\textbf{Note.}
For our main result we use a self-normalized martingale CLT; for background see \citet{hall2014martingale} and \citet{delapena2009self}. We include the specific self-normalized statement used in the proof of Theorem~\ref{thm:studentized} for completeness.

\begin{theorem}[Self-normalized martingale CLT]\label{thm:selfnorm}
Let $\{\xi_t,\mathcal{F}_t\}_{t=1}^n$ be an MDS array, with
$S_n=\sum_{t=1}^n\xi_t$, $V_n^2=\sum_{t=1}^n\E[\xi_t^2\mid\mathcal{F}_{t-1}]$,
and $Q_n=\sum_{t=1}^n\xi_t^2$. If (i) $V_n^2\pto\infty$, (ii) $Q_n/V_n^2\pto 1$,
and (iii) for some $\delta>0$,
$\sum_{t=1}^n\E[|\xi_t|^{2+\delta}]/V_n^{2+\delta}\to 0$, then
$S_n/\sqrt{Q_n}\dto \mathcal{N}(0,1)$.
\end{theorem}

\section{Assumption-to-Result Map}\label{app:assumptions}

Table~\ref{tab:assumption_ledger} is the assumption-to-result map.

\section{Data contract (minimum logging requirements)}\label{app:repro}
\begin{center}
\small
\begin{tabular}{@{}p{0.35\textwidth}p{0.58\textwidth}@{}}
\toprule
\textbf{Minimum log fields (per $t$)} & \textbf{Auditable invariants} \\
\midrule
(i) ordered index $t$;\quad
(ii) covariates $X_t$ as used by the assignment rule;\quad
(iii) treatment $A_t$;\quad
(iv) outcome $Y_t$;\quad
(v) executed probability $\pi_t$;\quad
(vi) overlap rule and $\varepsilon$ (if clipping) &
(a) $\pi_t$ equals the probability passed to the RNG that generated $A_t$;\quad
(b) overlap enforced at execution (no post-hoc truncation);\quad
(c) within a scored block, the nuisance object used is identical across its units;\quad
(d) the scored set $\mathcal{T}$ is fixed ex ante (this note) or predictable from past data (extensions). \\
\bottomrule
\end{tabular}
\end{center}

\section{Multi-arm extension (sketch)}\label{app:multiarm}
For $A_t\in\{0,1,\dots,J\}$ with logged propensities
$\pi_{t,j}=\Prob(A_t=j\mid\cG_t)$ and overlap
$\inf_{t,j}\pi_{t,j}\ge \varepsilon$, define $\theta_j=\E[Y(j)-Y(0)]$ and
\[
\psi_{t,j}(\theta_j; m, \pi)
=
\frac{\1\{A_t=j\}(Y_t-m_j(X_t))}{\pi_{t,j}}
-\frac{\1\{A_t=0\}(Y_t-m_0(X_t))}{\pi_{t,0}}
+m_j(X_t)-m_0(X_t)-\theta_j.
\]
With predictable nuisance fits, the same conditioning argument as
Lemma~\ref{lem:mds} gives an MDS for each coordinate, and the studentized CLT
applies componentwise.

% ============================================================================
% REFERENCES
% ============================================================================
\begin{thebibliography}{99}

\bibitem[Chernozhukov et~al.(2018)]{chernozhukov2018double}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018).
\newblock Double/debiased machine learning for treatment and structural parameters.
\newblock \emph{The Econometrics Journal}, 21(1):C1--C68.

\bibitem[Chernozhukov et~al.(2022)]{chernozhukov2022locally}
Chernozhukov, V., Newey, W.~K., and Singh, R. (2022).
\newblock Automatic debiased machine learning of causal and structural effects.
\newblock \emph{Econometrica}, 90(3):967--1027.

\bibitem[Cook et~al.(2024)]{cook2024semiparametric}
Cook, T., Mishler, A., and Ramdas, A. (2024).
\newblock Semiparametric efficient inference in adaptive experiments.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning (ICML)}, PMLR 235:9327--9361.

\bibitem[de la Pe{\~n}a et~al.(2009)]{delapena2009self}
de la Pe{\~n}a, V.~H., Lai, T.~L., and Shao, Q.-M. (2009).
\newblock \emph{Self-Normalized Processes: Limit Theory and Statistical Applications}.
\newblock Springer.

\bibitem[Hadad et~al.(2021)]{hadad2021confidence}
Hadad, V., Hirshberg, D.~A., Zhan, R., Wager, S., and Athey, S. (2021).
\newblock Confidence intervals for policy evaluation in adaptive experiments.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118(15):e2014602118.

\bibitem[Hahn et~al.(2011)]{hahnHiranoKarlan2011}
Hahn, J., Hirano, K., and Karlan, D. (2011).
\newblock Adaptive experimental design using the propensity score.
\newblock \emph{Journal of Business \& Economic Statistics}, 29(1):96--108.

\bibitem[Hall and Heyde(2014)]{hall2014martingale}
Hall, P. and Heyde, C.~C. (2014).
\newblock \emph{Martingale Limit Theory and Its Application}.
\newblock Academic Press.

\bibitem[Hirano and Porter(2023)]{hiranoPorter2023asymptotics}
Hirano, K. and Porter, J.~R. (2023).
\newblock Asymptotic representations for sequential decisions, adaptive experiments, and batched bandits.
\newblock Working paper, University of Texas at Austin and University of Wisconsin--Madison.

\bibitem[Kato et~al.(2020)]{kato2020adaptive}
Kato, M., Ishihara, K., Honda, J., and Narita, Y. (2020).
\newblock Adaptive treatment assignment in experiments for policy choice.
\newblock \emph{arXiv preprint arXiv:2005.00458}.

\bibitem[Li and Owen(2024)]{liOwen2024batch}
Li, X. and Owen, A.~B. (2024).
\newblock Double machine learning for adaptive causal inference from batched experiments.
\newblock Working paper, Stanford University.

\bibitem[Rosenberger and Lachin(2015)]{rosenberger2015randomization}
Rosenberger, W.~F. and Lachin, J.~M. (2015).
\newblock \emph{Randomization in Clinical Trials: Theory and Practice}.
\newblock John Wiley \& Sons.

\bibitem[Villar et~al.(2015)]{villar2015bandit}
Villar, S.~S., Bowden, J., and Wason, J. (2015).
\newblock Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges.
\newblock \emph{Statistical Science}, 30(2):199--215.

\bibitem[Zhan et~al.(2021)]{zhan2021off}
Zhan, R., Hadad, V., Hirshberg, D.~A., and Athey, S. (2021).
\newblock Off-policy evaluation via adaptive weighting with data from contextual bandits.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference}, pages 2125--2135.

\bibitem[Zhang et~al.(2020)]{zhang2020inference}
Zhang, K., Janson, L., and Murphy, S. (2020).
\newblock Inference for batched bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:9818--9829.

\end{thebibliography}

\end{document}
